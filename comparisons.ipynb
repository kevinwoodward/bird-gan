{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization, UpSampling2D, Dense, MaxPool2D, LeakyReLU, Reshape, Dropout, Flatten, GaussianNoise\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(img):\n",
    "    return (img - 127.5)/127.5\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    return (img + 1.) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = []\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"./CUB_200_2011/CUB_200_2011/images-cleaned/\", topdown=False):\n",
    "    for name in files:\n",
    "        im = cv2.cvtColor(cv2.imread(os.path.join(root, name)), cv2.COLOR_RGB2BGR)\n",
    "        im = cv2.resize(im, (75, 75))\n",
    "        train_imgs.append(im)\n",
    "\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_imgs = (train_imgs - 127.5)/127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model_mse(shape):\n",
    "    \n",
    "    input_layer = Input(shape=shape)\n",
    "    \n",
    "    x = Dense(8*8*64)(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Reshape((8, 8, 64))(x)\n",
    "    \n",
    "    x = Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)\n",
    "    \n",
    "    return Model(input_layer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model_mse():\n",
    "    input_layer = Input((64, 64, 3))\n",
    "    \n",
    "    x = Conv2D(64, (5, 5), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "    \n",
    "    return Model(input_layer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model_deeper(shape):\n",
    "    \n",
    "    input_layer = Input(shape=shape)\n",
    "    \n",
    "    x = Dense(16*16*64)(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Reshape((16, 16, 64))(x)\n",
    "    \n",
    "    x = GaussianNoise(1)(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = GaussianNoise(1)(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = GaussianNoise(1)(x)\n",
    "    x = Conv2DTranspose(16, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = GaussianNoise(1)(x)\n",
    "    x = Conv2DTranspose(16, (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(3, (3, 3), strides=(1, 1), padding='same', use_bias=False, activation='tanh')(x)\n",
    "    \n",
    "    return Model(input_layer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model_deeper():\n",
    "    input_layer = Input((64, 64, 3))\n",
    "    x = GaussianNoise(1)(input_layer)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(1)(x)\n",
    "    \n",
    "    return Model(input_layer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeper_g_200z = make_generator_model_deeper(200)\n",
    "deeper_g_100z = make_generator_model_deeper(100)\n",
    "# mse_g = make_generator_model_mse(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeper_g_200z.load_weights('./results/weights/bird-gan-deeper/gen_weights_deeper_4000.h5')\n",
    "deeper_g_100z.load_weights('./results/weights/bird-gan-deeper-smaller-z/gen_weights_deeper_smaller_z_4000.h5')\n",
    "# mse_g.load_weights('./results/weights/bird-gan-mse/gen_weights_mse_3900.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    noise200 = np.random.normal(0, 1, 200)\n",
    "    noise100 = noise200[:100]\n",
    "    \n",
    "    deeper_image_200z = deeper_g_200z.predict(np.array([noise200]))[0]\n",
    "    deeper_image_100z = deeper_g_100z.predict(np.array([noise100]))[0]\n",
    "    mse_image = mse_g.predict(np.array([noise200]))[0]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax1 = fig.add_subplot(131)\n",
    "    ax1.imshow(unnormalize_img(deeper_image_200z))\n",
    "    ax2 = fig.add_subplot(132)\n",
    "    ax2.imshow(unnormalize_img(deeper_image_100z))\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    ax3.imshow(unnormalize_img(mse_image))\n",
    "    ax1.title.set_text('DCGAN Z=200')\n",
    "    ax2.title.set_text('DCGAN Z=100')\n",
    "    ax3.title.set_text('DCGAN w/MSE Z=200')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_images(model, samples, noise_dim):\n",
    "    noise = np.random.normal(0, 1, (samples, noise_dim))\n",
    "    imgs = model.predict(noise)\n",
    "    resized_imgs = []\n",
    "    for img in imgs:\n",
    "        resized_imgs.append(cv2.resize(img, (75, 75)))\n",
    "    return np.array(resized_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images1, images2):\n",
    "    act1 = model.predict(images1)\n",
    "    act2 = model.predict(images2)\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = scipy.linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # calculate score\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv3 = create_upsampling_iv3()\n",
    "iv3 = InceptionV3(include_top=False, pooling='avg', input_shape=(75,75,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(train_imgs)\n",
    "real_images = train_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_images = generate_fake_images(deeper_g_200z, len(real_images), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = calculate_fid(iv3, real_images, fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240.56184273867197\n",
      "245.4820533514757\n",
      "246.48606891528502\n",
      "246.7581613690587\n",
      "246.3750956737907\n",
      "246.3750956737907\n"
     ]
    }
   ],
   "source": [
    "# For Z = 200\n",
    "fids = 0\n",
    "for i in range(5):\n",
    "    fake_images = generate_fake_images(deeper_g_200z, len(real_images), 200)\n",
    "    fids += calculate_fid(iv3, real_images, fake_images)\n",
    "    print(fids / (i + 1.))\n",
    "print(fids/5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.70468751120058\n",
      "209.29314922400238\n",
      "208.67185644913025\n",
      "209.90114872099065\n",
      "208.69270887096872\n",
      "208.69270887096872\n"
     ]
    }
   ],
   "source": [
    "# For Z = 100\n",
    "fids = 0\n",
    "for i in range(5):\n",
    "    fake_images = generate_fake_images(deeper_g_100z, len(real_images), 100)\n",
    "    fids += calculate_fid(iv3, real_images, fake_images)\n",
    "    print(fids / (i + 1.))\n",
    "print(fids/5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
