{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, BatchNormalization, UpSampling2D, Dense, MaxPool2D, LeakyReLU, Reshape, Dropout, Flatten, GaussianNoise, Embedding, multiply, concatenate\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(img):\n",
    "    return (img - 127.5)/127.5\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    return (img + 1.) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classes = ['chihuahua', 'chimpanzee', 'dalmatian', 'dolphin', 'fox', 'giant+panda', 'giraffe', 'otter', 'polar+bear', 'zebra']\n",
    "min_images = float('inf')\n",
    "for root, dirs, files in os.walk(\"./AwA2/AwA2-data/Animals_with_Attributes2/JPEGImagesCleaned/\", topdown=False):\n",
    "    if any(x in root for x in img_classes):\n",
    "        min_images = min(min_images, len(files))\n",
    "print('Minimum images across all included classes is:', min_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_imgs = []\n",
    "train_classes = []\n",
    "class_index = 0\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"./AwA2/AwA2-data/Animals_with_Attributes2/JPEGImagesCleaned/\", topdown=False):\n",
    "    print(f'{len(files)} samples in class {class_index}, ({root})')\n",
    "    if any(x in root for x in img_classes):\n",
    "        for name in files[:min_images]:\n",
    "            train_imgs.append(cv2.cvtColor(cv2.imread(os.path.join(root, name)), cv2.COLOR_RGB2BGR))\n",
    "            train_classes.append(class_index)\n",
    "        plt.imshow(train_imgs[-1])\n",
    "        plt.title(img_classes[class_index])\n",
    "        plt.show()\n",
    "        class_index += 1\n",
    "    if class_index >= 10:\n",
    "        break\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_imgs = (train_imgs - 127.5)/127.5\n",
    "train_classes = np.array(train_classes, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_classes, bins=10)\n",
    "plt.show()\n",
    "assert(len(train_imgs) == len(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(shape):\n",
    "    \n",
    "    noise_in = Input(shape=shape)\n",
    "    label_in = Input(shape=(1,))\n",
    "    label_embedding = Embedding(10, 100)(label_in)\n",
    "    \n",
    "    input_layer = multiply([noise_in, label_embedding])\n",
    "    \n",
    "    x = Dense(16*16*64*2)(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Reshape((16, 16, 128))(x)\n",
    "    \n",
    "    x = GaussianNoise(1)(x)\n",
    "    x = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = GaussianNoise(1)(x)\n",
    "    x = Conv2DTranspose(128, (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = GaussianNoise(1)(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = GaussianNoise(1)(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Conv2DTranspose(3, (3, 3), strides=(1, 1), padding='same', use_bias=False, activation='tanh')(x)\n",
    "    \n",
    "    return Model([noise_in, label_in], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model((100,))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model((100,))\n",
    "\n",
    "noise = np.random.normal(0, 1, 100)\n",
    "label = 1\n",
    "\n",
    "generated_image = generator.predict( [np.array([noise]), np.array([label])] )[0]\n",
    "plt.imshow(unnormalize_img(generated_image), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    input_layer = Input((64, 64, 3))\n",
    "    x = GaussianNoise(1)(input_layer)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    valid = Dense(1, activation='sigmoid')(x)\n",
    "    label = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    return Model(input_layer, [valid, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()\n",
    "valid, label = discriminator.predict(np.array([generated_image]))\n",
    "print(valid)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def discriminator_valid_loss(real_output, fake_output, flip):\n",
    "    \n",
    "    if flip:\n",
    "        fake_labels = tf.random.uniform(fake_output.shape, minval=0.9, maxval=1.0)\n",
    "        real_labels = tf.random.uniform(real_output.shape, minval=0.0, maxval=0.1)\n",
    "    else:\n",
    "        real_labels = tf.random.uniform(real_output.shape, minval=0.9, maxval=1.0)\n",
    "        fake_labels = tf.random.uniform(fake_output.shape, minval=0.0, maxval=0.1)\n",
    "\n",
    "        \n",
    "    real_loss = cross_entropy(real_labels, real_output)\n",
    "    fake_loss = cross_entropy(fake_labels, fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss, real_loss, fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_cce = SparseCategoricalCrossentropy(from_logits=False)\n",
    "cce = CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "'''\n",
    "real_classes, fake_classes: List of class ints (0-200)\n",
    "real_output, fake_output: List of softmax vectors\n",
    "'''\n",
    "def discriminator_class_loss_real(real_output, real_classes):\n",
    "    return sparse_cce(real_classes, real_output) \n",
    "\n",
    "def discriminator_class_loss_fake(fake_output, fake_classes):\n",
    "    return sparse_cce(fake_classes, fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = Adam(1e-4)\n",
    "discriminator_optimizer = Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels, flip, warmup):\n",
    "    noise = tf.random.truncated_normal([32, 100])\n",
    "    sampled_labels = tf.cast(tf.random.uniform([32, 1], minval=0, maxval=10, dtype='int32'), 'float32')\n",
    "#     sampled_labels += tf.random.truncated_normal(sampled_labels.shape, mean=0.0, stddev=0.05)\n",
    "    labels = tf.reshape(labels, (labels.shape[0], 1))\n",
    "#     labels += tf.random.truncated_normal(sampled_labels.shape, mean=0.0, stddev=0.05)\n",
    "    \n",
    "    \n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "    disc_real_losses = []\n",
    "    disc_fake_losses = []\n",
    "    disc_class_real_losses = []\n",
    "    disc_class_fake_losses = []\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator([noise, sampled_labels], training=True)\n",
    "        \n",
    "        real_output, real_label_output = discriminator(images, training=True)\n",
    "        fake_output, fake_label_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss, disc_real_loss, disc_fake_loss = discriminator_valid_loss(real_output, fake_output, flip)\n",
    "        disc_class_real_loss = discriminator_class_loss_real(real_label_output, labels)\n",
    "        disc_class_fake_loss = discriminator_class_loss_fake(fake_label_output, sampled_labels)\n",
    "        disc_loss += ((2.0 - warmup) * disc_class_real_loss + warmup * disc_class_fake_loss)\n",
    "        \n",
    "        gen_losses.append(K.mean(gen_loss))\n",
    "        disc_losses.append(K.mean(disc_loss))\n",
    "        disc_real_losses.append(K.mean(disc_real_loss))\n",
    "        disc_fake_losses.append(K.mean(disc_fake_loss))\n",
    "        disc_class_real_losses.append(K.mean(disc_class_real_loss))\n",
    "        disc_class_fake_losses.append(K.mean(disc_class_fake_loss))\n",
    "    \n",
    "    \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    \n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return (gen_losses, disc_losses, disc_real_losses, disc_fake_losses, disc_class_real_losses, disc_class_fake_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_imgs, train_labels, epochs, warmup):\n",
    "    gen_loss_all = []\n",
    "    disc_loss_all = []\n",
    "    disc_loss_real_all = []\n",
    "    disc_loss_fake_all = []\n",
    "    disc_class_loss_real_all = []\n",
    "    disc_class_loss_fake_all = []\n",
    "    warmup_step = 1./5000.\n",
    "    \n",
    "    num_samples = len(train_imgs)\n",
    "    batch_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        gen_loss_epoch = [1, 0]\n",
    "        disc_loss_epoch = [1, 0]\n",
    "        disc_loss_real_epoch = [1, 0]\n",
    "        disc_loss_fake_epoch = [1, 0]\n",
    "        disc_class_loss_real_epoch = [1, 0]\n",
    "        disc_class_loss_fake_epoch = [1, 0]\n",
    "        \n",
    "        seed = np.random.randint(0, 10000)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(train_imgs)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(train_labels)\n",
    "        \n",
    "        for idx, i in enumerate(range(0, num_samples - 32, 32)):\n",
    "            \n",
    "            gen_loss_batch, disc_loss_batch, disc_loss_real_batch, disc_loss_fake_batch, disc_class_loss_real_batch, disc_class_loss_fake_batch = train_step(train_imgs[i:(i+32)], train_labels[i:(i+32)], batch_counter % 20 == 0, tf.constant(warmup, dtype='float32'))\n",
    "\n",
    "            gen_loss_epoch[0] += 1\n",
    "            disc_loss_epoch[0] += 1\n",
    "            disc_loss_real_epoch[0] += 1\n",
    "            disc_loss_fake_epoch[0] += 1\n",
    "            disc_class_loss_real_epoch[0] += 1\n",
    "            disc_class_loss_fake_epoch[0] += 1\n",
    "            gen_loss_epoch[1] += np.mean(gen_loss_batch)\n",
    "            disc_loss_epoch[1] += np.mean(disc_loss_batch)\n",
    "            disc_loss_real_epoch[1] += np.mean(disc_loss_real_batch)\n",
    "            disc_loss_fake_epoch[1] += np.mean(disc_loss_fake_batch)\n",
    "            disc_class_loss_real_epoch[1] += np.mean(disc_class_loss_real_batch)\n",
    "            disc_class_loss_fake_epoch[1] += np.mean(disc_class_loss_fake_batch)\n",
    "            \n",
    "            batch_counter += 1\n",
    "            if warmup < 1:\n",
    "                warmup += warmup_step\n",
    "            \n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "        print(f'Gen loss: {gen_loss_epoch[1]/gen_loss_epoch[0]}, Disc loss: {disc_loss_epoch[1]/disc_loss_epoch[0]}, Disc valid real loss: {disc_loss_real_epoch[1]/disc_loss_real_epoch[0]}, Disc valid fake loss: {disc_loss_fake_epoch[1]/disc_loss_fake_epoch[0]}, Disc class real loss: {disc_class_loss_real_epoch[1]/disc_class_loss_real_epoch[0]}, Disc class fake loss: {disc_class_loss_fake_epoch[1]/disc_class_loss_fake_epoch[0]}')\n",
    "        \n",
    "        gen_loss_all.append(gen_loss_epoch[1]/gen_loss_epoch[0])\n",
    "        disc_loss_all.append(disc_loss_epoch[1]/disc_loss_epoch[0])\n",
    "        disc_loss_real_all.append(disc_loss_real_epoch[1]/disc_loss_real_epoch[0])\n",
    "        disc_loss_fake_all.append(disc_loss_fake_epoch[1]/disc_loss_fake_epoch[0])\n",
    "        disc_class_loss_real_all.append(disc_class_loss_real_epoch[1]/disc_class_loss_real_epoch[0])\n",
    "        disc_class_loss_fake_all.append(disc_class_loss_fake_epoch[1]/disc_class_loss_fake_epoch[0])\n",
    "        \n",
    "    \n",
    "    return (gen_loss_all, disc_loss_all, disc_loss_real_all, disc_loss_fake_all, disc_class_loss_real_all, disc_class_loss_fake_all)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_train(starting_checkpoint=0, num_checkpoints=0, checkpoint_interval=0, g_hist=None, d_hist=None, d_real_hist=None, d_fake_hist=None, d_class_real_hist=None, d_class_fake_hist=None, noise=None, label=None, save_files=False, warmup=0.):\n",
    "    \n",
    "#     if noise is None:\n",
    "#         noise = np.random.normal(0, 1, 100)\n",
    "\n",
    "    if g_hist is None:\n",
    "        g_hist = []\n",
    "    \n",
    "    if d_hist is None:\n",
    "        d_hist = []\n",
    "        \n",
    "    if d_real_hist is None:\n",
    "        d_real_hist = []\n",
    "    \n",
    "    if d_fake_hist is None:\n",
    "        d_fake_hist = []\n",
    "        \n",
    "    if d_class_real_hist is None:\n",
    "        d_class_real_hist = []\n",
    "        \n",
    "    if d_class_fake_hist is None:\n",
    "        d_class_fake_hist = []\n",
    "\n",
    "    for i in range(starting_checkpoint, starting_checkpoint + num_checkpoints):\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print('')\n",
    "        print(f'Starting checkpoint {i}')\n",
    "        print('')\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        \n",
    "        gen_temp, disc_temp, disc_real_temp, disc_fake_temp, disc_class_real_temp, disc_class_fake_temp = train(train_imgs, train_classes, checkpoint_interval, warmup)\n",
    "        g_hist += gen_temp\n",
    "        d_hist += disc_temp\n",
    "        d_real_hist += disc_real_temp\n",
    "        d_fake_hist += disc_fake_temp\n",
    "        d_class_real_hist += disc_class_real_temp\n",
    "        d_class_fake_hist += disc_class_fake_temp\n",
    "        generated_image = generator.predict( [np.array([noise]), np.array([label])])[0]\n",
    "        plt.imshow(unnormalize_img(generated_image))\n",
    "        if save_files:\n",
    "            plt.savefig('./results/prog-imgs/animal-gan-conditional-10-samples/' + str((i+1)*checkpoint_interval) + '.png')\n",
    "        plt.show()\n",
    "        if save_files:\n",
    "            generator.save_weights('./results/weights/animal-gan-conditional-10-samples/gen_weights_conditional_10_samples' + str((i+1)*checkpoint_interval) + '.h5')\n",
    "            discriminator.save_weights('./results/weights/animal-gan-conditional-10-samples/disc_weights_conditional_10_samples' + str((i+1)*checkpoint_interval) + '.h5')\n",
    "    \n",
    "    return starting_checkpoint + num_checkpoints, totalepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "generator = make_generator_model((100,))\n",
    "generator_optimizer = Adam(1e-4)\n",
    "discriminator_optimizer = Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get passed into training function and get modified\n",
    "# Run cell when starting from scratch\n",
    "gen_loss_hist = []\n",
    "disc_loss_hist = []\n",
    "disc_loss_real_hist = []\n",
    "disc_loss_fake_hist = []\n",
    "disc_class_loss_real_hist = []\n",
    "disc_class_loss_fake_hist = []\n",
    "noise100 = np.random.normal(0, 1, 100)\n",
    "label = 1\n",
    "next_starting_checkpoint = 0\n",
    "totalepochs = 0\n",
    "warmup = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(label)\n",
    "next_starting_checkpoint, totalepochs = long_train(starting_checkpoint=next_starting_checkpoint, \n",
    "                                      num_checkpoints=40, \n",
    "                                      checkpoint_interval=250, \n",
    "                                      g_hist=gen_loss_hist, \n",
    "                                      d_hist=disc_loss_hist,\n",
    "                                      d_real_hist=disc_loss_real_hist,\n",
    "                                      d_fake_hist=disc_loss_fake_hist,\n",
    "                                      d_class_real_hist=disc_class_loss_real_hist,\n",
    "                                      d_class_fake_hist=disc_class_loss_fake_hist,\n",
    "                                      noise=noise100,\n",
    "                                      label=label,\n",
    "                                      save_files=True,\n",
    "                                      warmup=warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_weights('./results/weights/animal-gan-conditional-10-samples/gen_weights_conditional_10_samples10000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(img_classes)\n",
    "# cls = 5\n",
    "noise = np.random.normal(0, 1, 100)\n",
    "for label in range(10):\n",
    "    generated_image = generator.predict([np.array([noise]), np.array([label])] )[0]\n",
    "    plt.imshow(unnormalize_img(generated_image))\n",
    "    plt.title(img_classes[label])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gen_loss_hist)\n",
    "plt.plot(disc_loss_hist)\n",
    "\n",
    "plt.legend(['Gen Loss','Disc Loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(gen_loss_hist - np.mean(gen_loss_hist))\n",
    "plt.plot(disc_loss_hist - np.mean(disc_loss_hist))\n",
    "plt.legend(['Gen Loss Normalized','Disc Loss normalized'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(disc_loss_real_hist)\n",
    "plt.plot(disc_loss_fake_hist)\n",
    "plt.legend(['Disc Real Loss','Disc Fake Loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(disc_class_loss_real_hist)\n",
    "plt.plot(disc_class_loss_fake_hist)\n",
    "plt.legend(['Disc Class Real Loss', 'Disc Class Fake Loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.truncated_normal([32, 100])\n",
    "tf.dtypes.cast(noise, tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20,30):\n",
    "    plt.imshow(unnormalize_img(train_imgs[i]))\n",
    "    plt.title(img_classes[int(train_classes[i])])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer_model = Model(inputs=generator.input[1],\n",
    "                                 outputs=generator.layers[2].output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = None\n",
    "for label in range(10):\n",
    "    if total is None:\n",
    "        total = np.abs(intermediate_layer_model.predict(np.array([label]))[0][0])\n",
    "    total += np.abs(intermediate_layer_model.predict(np.array([label]))[0][0])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer_model.predict(np.array([0]))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = make_generator_model((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_embedding = Model(inputs=test_gen.input[1],\n",
    "                                 outputs=test_gen.layers[2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_embedding.predict(np.array([7]))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
